---
title: "ABC Beverage: Predictive pH Factors Research Spike"
author: "Tony Fraser and Seung-min Song"
date: "8 May 2024"
format:
  html:
    theme: cosmo
    toc: true
    number_sections: true
---

```{r load_libraries, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(ggplot2)
library(corrplot)
library(patchwork)
library(shiny)
library(caret)
library(randomForest)
library(tidyr)
```

In this research spike we will drill into our manufacturing process and attempt to build models to model PH balance in our product. 

# Executive Summary
This project aims to establish a robust predictive model for pH levels in the bottling process. We explored various modeling techniques, with the Random Forest model employing cross-validation emerging as the standout for its accuracy and resilience.

## Key Outcomes
* Our best-performing models are:
    After model tuning: Random Forest with an R Squared of 0.6917606.
    After implementing select outlier removals: Random Forest with an R Squared of 0.9763720.
* Approximately 30% of the columns are either irrelevant or collinear.
* Regarding Random Forest, an 80/20 test/train split yielded the best results.
* We opted to make predictions based on the model without outliers moved. This can be easily swapped to the model where outliers are included.

## What's Next or Missing
* We lack a data dictionary and industry knowledge, which are essential for understanding the meaning of these columns.
* Some fields in the data are empty. We excluded them during model building, but we lack information to determine if this was the correct approach.
* There is variance and outliers in the data, with some outliers being significant. To improve model stability, we removed a few training records but we are uncertain if this is a prudent decision. We kept both models in this study. 

# Data Overview 
1. Our sample consists of about 2800 entries with 33 columns.
2. This is not time-series data, just a collection of points.
3. Much of the data is about measured points at the time of bottling.
4. pH is the column we are going to try to model.
5. The StudentEvaluation_Test is not a train/test dataset. It has no pH values in it. This dataset is for predicting when we’re all done. We’ll call it the predict_me dataset.
6. On outliers: We’re choosing not to remove them. Random Forest doesn’t seem to mind outliers, also these are data points within bottling. Each one is important, and there doesn’t seem to be anything so extreme that it should be removed.

## Load Data
We moved data up to github and will load straight from there. Immediately on loading, column names will be cleaned and standardized. No spaces, all lower case, etc. 

```{r loadData, message=FALSE, warning=FALSE, results='hide'}
github_url <- "https://github.com/tonythor/abc-beverage/raw/develop/data/"
train_fn <- "StudentData_Training.xlsx"
predict_me_fn <- "StudentEvaluation_Test.xlsx"
train_url <- paste0(github_url, train_fn) 
predict_me_url <- paste0(github_url, predict_me_fn)
download.file(train_url, destfile = train_fn, mode = "wb")
download.file(predict_me_url, destfile = predict_me_fn, mode = "wb")

# read in, clean by replacing all spaces and upper case column names
# and mutate any nulls with a column average. 
# train_raw has 2571 records.
train_raw <- read_excel(train_fn) %>%
  rename_with(~ gsub(" ", "", .x) %>% tolower()) %>%
  mutate(brandcodenumeric = as.numeric(as.factor(brandcode))) %>%
  select(-brandcode) %>%
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))  # Convert all factors to numeric

#train has 2038 records
train <- train_raw %>%  filter_all(all_vars(!is.na(.)))

predict_me <- read_excel(predict_me_fn) %>%
  rename_with(~ gsub(" ", "", .x) %>% tolower()) %>%
  mutate(brandcodenumeric = as.numeric(as.factor(brandcode))) %>%
  select(-brandcode) %>% 
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))  # Convert all factors to numeric

file.remove(c(train_fn, predict_me_fn))
```

## Basic visualizations 

PH seems to be fairly normally distributed.  After looking at a correlation chart, we might have some colinearity to deal depending on the model we select.

```{r basicCharts, fig.width=12, fig.height=6, message=FALSE, warning=FALSE}
numeric_data <- train %>% select_if(is.numeric)

#correlation matrix to df
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_df <- as.data.frame(as.table(cor_matrix))
names(cor_df) <- c("Variable1", "Variable2", "Correlation")

# Filter to include only higher correlations
threshold <- 0.5
cor_df <- cor_df %>%
  filter(abs(Correlation) >= threshold, Variable1 != Variable2)

cor_plot <- ggplot(cor_df, aes(x = Variable1, y = Variable2, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.text.y = element_text(angle = 0)) +
  labs(fill = "Correlation", title = "Filtered Correlation Matrix")

# Check if pH is numeric and plot its distribution
if("ph" %in% names(numeric_data)) {
  ph_plot <- ggplot(numeric_data, aes(x = ph)) + 
    geom_histogram(bins = 30, fill = "blue", color = "black") +
    ggtitle("Distribution of pH Values") +
    xlab("pH") +
    ylab("Frequency")
} else {
  # Creating an empty plot if pH is not present or not numeric
  ph_plot <- ggplot() +
    geom_blank() +
    ggtitle("No pH Data Available")
}

# Combine the plots side by side using patchwork
combined_plot <- ph_plot + cor_plot

# Display the combined plot
print(combined_plot)
```
## Shiny app: boxpolots
You can review all the box and whisker plots for each numeric variable via this shiny app. 

```{r html_link, echo=FALSE, results='asis'}
cat('<a href="https://afraser.shinyapps.io/shiny/" target="_blank" onclick="window.open(\'https://afraser.shinyapps.io/shiny/\', \'newwindow\', \'width=600,height=600\'); return false;">Launch the popup</a> to review individual variable boxplots.')
```


# Models quick look
Let's do some fit and predictions of several more common models, and see how they perform. Of the metrics at the bottom of this section, the one with the best RSquared is the one we are likely to select.

```{r modelCompare, , message=FALSE, warning=FALSE}

set.seed(200)  # for reproducibility

train <- train %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%
  select(where(is.numeric))  # Ensure only numeric data is used

# Split the train data into new training and testing sets
set.seed(123)  # Ensure reproducibility
indexes <- createDataPartition(train$ph, p = 0.8, list = FALSE)
training_data <- train[indexes, ]
testing_data <- train[-indexes, ]

# Prepare predictor and response variables correctly
training_data$x <- data.frame(sapply(training_data[, -which(names(training_data) == "ph")], as.numeric))
training_data$y <- training_data$ph
testing_data$x <- data.frame(sapply(testing_data[, -which(names(testing_data) == "ph")], as.numeric))
testing_data$y <- testing_data$ph

# KNN
knnModel <- train(x = training_data$x, y = training_data$y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 5)
knnPred <- predict(knnModel, newdata = testing_data$x)
knnMetrics <- postResample(pred = knnPred, obs = testing_data$y)

# MARS
marsModel <- train(x = training_data$x, y = training_data$y,
                   method = "earth",
                   preProc = c("center", "scale"),
                   tuneLength = 5)
marsPred <- predict(marsModel, newdata = testing_data$x)
marsMetrics <- postResample(pred = marsPred, obs = testing_data$y)

# Neural Net
nnModel <- train(x = training_data$x, y = training_data$y,
                 method = "nnet",
                 preProcess = c("center", "scale"),
                 tuneLength = 5, 
                 trace = FALSE)
nnPred <- predict(nnModel, newdata = testing_data$x)
nnMetrics <- postResample(pred = nnPred, obs = testing_data$y)

# SVM
svmModel <- train(x = training_data$x, y = training_data$y,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneLength = 5)
svmPred <- predict(svmModel, newdata = testing_data$x)
svmMetrics <- postResample(pred = svmPred, obs = testing_data$y)

# Random Forest
## features added in for importance after first round assesment. 
trainControl <- trainControl(
  method = "cv", # <- this is how you get your model to cross validate!
  number = 3, # <- three fold validation
  verboseIter = FALSE,
  savePredictions = "final",
  returnResamp = "all"
)

tuneGrid <- expand.grid(
  mtry = c(2, floor(sqrt(ncol(training_data$x))))
)

rfModel <- train(
  x = training_data$x,
  y = training_data$y,
  method = "rf",
  trControl = trainControl,
  tuneGrid = tuneGrid
)
rfPred <- predict(rfModel, newdata = testing_data$x)
rfMetrics <- postResample(pred = rfPred, obs = testing_data$y)
importance_measures <- varImp(rfModel, scale = TRUE)

# Linear Regression
lmModel <- train(x = training_data$x, y = training_data$y,
                 method = "lm",
                 preProcess = c("center", "scale"))
lmPred <- predict(lmModel, newdata = testing_data$x)
lmMetrics <- postResample(pred = lmPred, obs = testing_data$y)

# Collecting all model performance metrics into a small df
modelPerformance <- data.frame(
  Model = c("KNN", "MARS", "Neural Network", "SVM", "Random Forest (RF)", "Linear Regression (LR)"),
  RMSE = c(knnMetrics[1], marsMetrics[1], nnMetrics[1], svmMetrics[1], rfMetrics[1], lmMetrics[1]),
  Rsquared = c(knnMetrics[2], marsMetrics[2], nnMetrics[2], svmMetrics[2], rfMetrics[2], lmMetrics[2]),
  MAE = c(knnMetrics[3], marsMetrics[3], nnMetrics[3], svmMetrics[3], rfMetrics[3], lmMetrics[3])
)



print(modelPerformance, row.names = FALSE)
```

# Expanding Random Forest
We know Random Forest is going to fit better, but before we predict, let's go through all the details and refine.

## Feature Importance
First, let's get a handle on which features seem the most important. Wonderful.

```{r expandRandomForest, message=FALSE, warning=FALSE}
importance_measures <- varImp(rfModel, scale = TRUE)
plot(importance_measures, main = "Feature Importance in Random Forest Model")
```
## Removing irrelevant columns
We have a bunch of columns that confuse the model becuase they are irrlevant. Before we refit this model, we'll set an importance threshold to help us get rid of all those columns. Then, we'll build a new model, refit, and see how well it performs. 

Not bad. It's over 69%. If we were rounding, it'd be good enough.

```{r rfRemoveColumms, message=FALSE, warning=FALSE, results='hide'}
# Function to clean and type
clean_data <- function(data, vars_to_remove) {
  data <- data[, !(names(data) %in% vars_to_remove)]
  data <- as.data.frame(data)
  data[] <- lapply(data, function(x) if(is.list(x)) unlist(x) else x)
  return(data)
}

# important features
importance_df <- as.data.frame(importance_measures$importance)
importance_df$Variable <- rownames(importance_df)
importance_df <- importance_df[order(importance_df$Overall), ]
# print(importance_df)

# cut off at less than importance_factor %
importance_factor <- 0.3
cutoff <- quantile(importance_df$Overall, importance_factor)
variables_to_remove <- importance_df$Variable[importance_df$Overall <= cutoff]

training_data_updated <- clean_data(training_data, c(variables_to_remove, "x", "y", "ph"))
# check the data types of variables after cleaning and removal.
# sapply(training_data_updated, class)

trainControl2 <- trainControl(
  method = "cv", # <- this is how you get your model to cross validate!
  number = 3, # <- three fold validation
  verboseIter = FALSE,
  savePredictions = "final",
  returnResamp = "all"
)

tuneGrid2 <- expand.grid(
  mtry = c(2, floor(sqrt(ncol(training_data$x))))
)

n_tree = 2000
rfModel_updated <- train(
  x = training_data_updated,
  y = training_data$y,
  method = "rf",
  trControl = trainControl2,
  tuneGrid = tuneGrid2,
  ntree = n_tree
)

# prepare and clean prediction data
predictor_data_for_model <- clean_data(testing_data, c(variables_to_remove, "x", "y"))
# sapply(predictor_data_for_model, class)  # Optional: Check classes of the prediction data columns

# Make predictions and evaluate the model
rfPred_updated <- predict(rfModel_updated, newdata = predictor_data_for_model)
rfMetrics_updated <- postResample(pred = rfPred_updated, obs = testing_data$y)

updated_model_performance <- data.frame(
  Model = paste0("RF (if:ntree ", importance_factor,":", n_tree,  ")"),
  RMSE = rfMetrics_updated[1],
  Rsquared = rfMetrics_updated[2],
  MAE = rfMetrics_updated[3]
)

# Note that at this point, we now have two rf modles, rfModel and rf_model_updated.
# Also, at this step, testing_updated and training_updated still have 1631/407 records.

modelPerformance <- rbind(modelPerformance, updated_model_performance)
print(modelPerformance, row.names = FALSE)

```

```{r outlierFiltering}
print(variables_to_remove) #<- these were removed before we re-trained our model 
print(modelPerformance, row.names = FALSE) #<- a big improvement
```

## Redsiduals Plot Review
The residuals appear randomly distributed around the zero line which is ideal. This suggests that the model does not suffer from non-constant variance. As well, there are no apparent patterns or trends in the residuals, which implies that the linearity between predictors and target is reasonably satisfied.  

There are some outliers. If this was our job, we'd be able to explain those. 


```{r residualsPlot}
predictions <- predict(rfModel_updated, newdata = predictor_data_for_model)
residuals <- testing_data$y - predictions
residuals_df <- data.frame(Predicted = predictions, Residuals = residuals)
residuals_df <- na.omit(residuals_df)
ggplot(residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")
```


## Outlier removal
Random Forest deals reasonably well with outliers, but let's kick out just a few here to stabilize our model a litlte more. We'll use an IQR strategy and remove only the furthest out of bounds, the ones that are way way out, like 8X the distance between quartile 1 and quartile 3. 

Randomly dropping records becuase they are outliers is not generally a preferred strategy, but we're already dropping a few records becuase they have null fields. Those might be valid records, we don't know. We also don't know if these records we're dropping are valid or not. We should look into each of them and figure out why, and try to account for that variance some other way.

From a process perspective, let's use our previous test/train split with unimportant columns removed. All we'll do is remove a few of the most records furthest from the mean. 

And obviously, this drastically stabilized the model as our performance table below shows.


```{r}
iqr_multiplier <- 5
filter_outliers <- function(data) {
  num_cols <- sapply(data, is.numeric)  # Identify numeric columns
  before_count <- nrow(data)
  data <- data %>%
    filter(if_all(where(is.numeric), ~ {
      q1 <- quantile(., 0.25, na.rm = TRUE)
      q3 <- quantile(., 0.75, na.rm = TRUE)
      iqr <- q3 - q1
      # lower bound(-25)= 25 -( IQRMultipler=1 * 50) < assuming 25/50/75/100 quartile ranges 
      lower_bound <- q1 - iqr_multiplier * iqr
      upper_bound <- q3 + iqr_multiplier * iqr
      . >= lower_bound & . <= upper_bound
    }))
  after_count <- nrow(data)
  return(list(data = data, before = before_count, after = after_count))
}

# Apply the outlier filter to training and testing data
filtered_training_data <- filter_outliers(training_data)
filtered_testing_data <- filter_outliers(testing_data)

# Extract data and counts
training_data_filtered <- filtered_training_data$data
testing_data_filtered <- filtered_testing_data$data
train_before <- filtered_training_data$before
train_after <- filtered_training_data$after
test_before <- filtered_testing_data$before
test_after <- filtered_testing_data$after

# Proceed with the cleaning of data
training_data_updated <- clean_data(training_data_filtered, c(variables_to_remove, "x", "y", "ph"))
testing_data_for_model <- clean_data(testing_data_filtered, c(variables_to_remove, "x", "y"))

# Ensure 'y' is still correctly aligned with your training data
training_data_updated$y <- training_data_filtered$ph
testing_data_for_model$y <- testing_data_filtered$ph

# Train the Random Forest model with the cleaned and filtered data
rfModel_filtered <- train(
  x = training_data_updated,
  y = training_data_updated$y,
  method = "rf",
  trControl = trainControl2,
  tuneGrid = tuneGrid2,
  ntree = n_tree
)

# Make predictions and evaluate the model
rfPred_filtered <- predict(rfModel_filtered, newdata = testing_data_for_model)
rfMetrics_filtered <- postResample(pred = rfPred_filtered, obs = testing_data_for_model$y)

# Store performance metrics
filtered_model_performance <- data.frame(
  Model = paste0("RF Filtered NoOutliers (if:ntree ", importance_factor, ":", n_tree, ")"),
  RMSE = rfMetrics_filtered[1],
  Rsquared = rfMetrics_filtered[2],
  MAE = rfMetrics_filtered[3]
)

modelPerformance <- rbind(modelPerformance, filtered_model_performance)
print(modelPerformance, row.names = FALSE)
```
## Record Removal Visualization

How many outliers do we have to remove to stabilize that model?

```{r}

# Create the data frame for plotting
counts_data <- data.frame(
  Stage = rep(c("Before", "After"), each = 2),
  Dataset = rep(c("Training", "Testing"), 2),
  Count = c(train_before, test_before, train_after, test_after)
)

# Adjust the order of levels for Stage to make 'Before' appear on the left and 'After' on the right
counts_data$Stage <- factor(counts_data$Stage, levels = c("Before", "After"))

# Plotting the bar chart with ggplot2
library(ggplot2)
ggplot(counts_data, aes(x = Stage, y = Count, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(
    title = "Record Counts Before and After Outlier Filtering",
    x = "",
    y = "Record Count",
    fill = "Dataset"
  ) +
  geom_text(aes(label = Count, group = Dataset), position = position_dodge(width = 0.9), vjust = -0.25)

```

# Generating Predictions 
## Imputation

Before we can predict, we need to do something with the missing fields from the predict_me data set. We'll use a simple linear regression model to find all those fields, go through the predictors, and fill those missing records out. Please note this is reusable code. 

```{r lrImputeMethod, message=FALSE, warning=FALSE}
impute_linear_regression <- function(data) {
    # For each column ... 
    for (col in names(data)) {
        # is numeric and any missing values
        if (is.numeric(data[[col]]) && any(is.na(data[[col]]))) {
            # Identify predictor columns, excluding the current column and any columns with missing values
            predictors <- setdiff(names(data), col)
            predictors <- predictors[!sapply(data[predictors], function(x) any(is.na(x)))]

            # Extract rows where the target column is not missing for training the model
            train_data <- data[!is.na(data[[col]]), predictors, drop = FALSE]
            train_target <- data[[col]][!is.na(data[[col]])]

            # if predictors and sufficient data, fit a linear model
            if (nrow(train_data) > 1 && length(predictors) > 0) {
                lm_model <- lm(train_target ~ ., data = train_data)

                # find indices of missing values in the target column
                na_indices <- which(is.na(data[[col]]))
                if (length(na_indices) > 0) {
                    # predict missing values
                    predictions <- predict(lm_model, newdata = data[na_indices, predictors, drop = FALSE])
                    # Replace missing values with the predictions
                    data[[col]][na_indices] <- predictions
                }
            } else {
                # Use median imputation as a fallback when not enough data is available for regression
                median_value <- median(train_target, na.rm = TRUE)
                data[[col]][is.na(data[[col]])] <- median_value
            }
        }
    }
    return(data)
}

```

## Predict
Now that we have an imputation strategy, we can predict. Please note in this research we have to Random Forest models, rfModel, and rfModel_updated. The updated one is the one where we further refined by adjusting and removing columns to get a better fit. We'll be using that one. 

The output file will be saved to `with_predictions.csv`, and the prediction column is `predicted_ph.` 

```{r rePredict, message=FALSE, warning=FALSE, results='hide'}
all_vars_to_remove <- c("ph", variables_to_remove) 

predict_me_filled <- predict_me %>% 
  select(-all_of(all_vars_to_remove)) %>% 
  impute_linear_regression()

predicted_ph <- predict(rfModel_updated, newdata = predict_me_filled)
predict_me_filled$predicted_ph <- round(predicted_ph, 5)

predict_me_filled %>% ## one last cleanup. 
  mutate_if(is.numeric, round, digits = 5)
  
write.csv(predict_me_filled, "with_predictions.csv", row.names = FALSE)
```

