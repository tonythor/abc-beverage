---
title: "ABC Beverage: Predictive PH Factors Research Spike"
author: "Tony Fraser and Seung-min Song"
date: "8 May 2024"
format:
  html:
    theme: cosmo
    toc: true
    number_sections: true
---

```{r load_libraries, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(ggplot2)
library(corrplot)
library(patchwork)
library(shiny)
library(caret)
library(randomForest)
```

In this research spike we will drill into our manufacturing process and attempt to build models to model PH balance in our product. 

# Executive Summary
This project aims to establish a robust predictive model for pH levels in the bottling process. We explored various modeling techniques, with the Random Forest model employing cross-validation emerging as the standout for its accuracy and resilience.

Key outcomes from the model refinement include:

* We tested several models before selecting Random Forest. Several were omitted from this final output.
* Our best performing model is a Random Forest model with an RSquared of 0.6963521
* All tested models perform better with null records dropped.
* About 30% of the columns are irrelevant for random Forest.

# Data Overview 
1. Our sample consists of about 2800 entries with 33 columns.
2. This is not time-series data, just a collection of points.
3. Much of the data is about measured points at the time of bottling.
4. pH is the column we are going to try to model.
5. The StudentEvaluation_Test is not a train/test dataset. It has no pH values in it. This dataset is for predicting when we’re all done. We’ll call it the predict_me dataset.
6. On outliers: We’re choosing not to remove them. Random Forest doesn’t seem to mind outliers, also these are data points within bottling. Each one is important, and there doesn’t seem to be anything so extreme that it should be removed.

## Load Data
We moved data up to github and will load straight from there. Immediately on loading, column names will be cleaned and standardized. No spaces, all lower case, etc. 

```{r loadData, message=FALSE, warning=FALSE, results='hide'}
github_url <- "https://github.com/tonythor/abc-beverage/raw/develop/data/"
train_fn <- "StudentData_Training.xlsx"
predict_me_fn <- "StudentEvaluation_Test.xlsx"
train_url <- paste0(github_url, train_fn) 
predict_me_url <- paste0(github_url, predict_me_fn)
download.file(train_url, destfile = train_fn, mode = "wb")
download.file(predict_me_url, destfile = predict_me_fn, mode = "wb")

# read in, clean by replacing all spaces and upper case column names
# and mutate any nulls with a column average. 
train_raw <- read_excel(train_fn) %>%
  rename_with(~ gsub(" ", "", .x) %>% tolower()) %>%
  mutate(brandcodenumeric = as.numeric(as.factor(brandcode))) %>%
  select(-brandcode) %>%
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))  # Convert all factors to numeric

train <- train_raw %>%  filter_all(all_vars(!is.na(.)))

predict_me <- read_excel(predict_me_fn) %>%
  rename_with(~ gsub(" ", "", .x) %>% tolower()) %>%
  mutate(brandcodenumeric = as.numeric(as.factor(brandcode))) %>%
  select(-brandcode) %>% 
  mutate(across(where(is.factor), ~ as.numeric(as.factor(.))))  # Convert all factors to numeric

train_imputed <- read_excel(train_fn) %>%
  rename_with(~ gsub(" ", "", .x) %>% tolower()) %>%
  mutate(brandcodenumeric = as.numeric(as.factor(brandcode))) %>%
  select(-brandcode)

# Create a data preprocessing object based on the remaining columns except the 'ph' column
preProc_without_ph <- preProcess(train[, setdiff(names(train), "ph")], method = "bagImpute")
train_imputed <- predict(preProc_without_ph, newdata = train[, setdiff(names(train), "ph")])
train_imputed$ph <- train$ph

file.remove(c(train_fn, predict_me_fn))
```

## Basic visualizations 

PH seems to be fairly normally distributed.  After looking at a correlation chart, we might have some colinearity to deal depending on the model we select.

```{r basicCharts, fig.width=12, fig.height=6, message=FALSE, warning=FALSE}
numeric_data <- train %>% select_if(is.numeric)

#correlation matrix to df
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_df <- as.data.frame(as.table(cor_matrix))
names(cor_df) <- c("Variable1", "Variable2", "Correlation")

# Filter to include only higher correlations
threshold <- 0.5
cor_df <- cor_df %>%
  filter(abs(Correlation) >= threshold, Variable1 != Variable2)

cor_plot <- ggplot(cor_df, aes(x = Variable1, y = Variable2, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.text.y = element_text(angle = 0)) +
  labs(fill = "Correlation", title = "Filtered Correlation Matrix")

# Check if pH is numeric and plot its distribution
if("ph" %in% names(numeric_data)) {
  ph_plot <- ggplot(numeric_data, aes(x = ph)) + 
    geom_histogram(bins = 30, fill = "blue", color = "black") +
    ggtitle("Distribution of pH Values") +
    xlab("pH") +
    ylab("Frequency")
} else {
  # Creating an empty plot if pH is not present or not numeric
  ph_plot <- ggplot() +
    geom_blank() +
    ggtitle("No pH Data Available")
}

# Combine the plots side by side using patchwork
combined_plot <- ph_plot + cor_plot

# Display the combined plot
print(combined_plot)
```
## Numeric variable boxpolots
You can review all the box and whisker plots for each numeric variable via this shiny app. 

```{r html_link, echo=FALSE, results='asis'}
cat('<a href="https://afraser.shinyapps.io/shiny/" target="_blank" onclick="window.open(\'https://afraser.shinyapps.io/shiny/\', \'newwindow\', \'width=600,height=600\'); return false;">Launch the popup</a> to review individual variable boxplots.')
```

# Modeling
When you compare several of the more common models, random forest will by far better fit and predict this data. As well, random forest deals with collinear variables already so we should be able to forecast straight from the trianint dataset. Still though, let's look a little deeper before we predict. 

## Model Overview
Let's do some fit and predictions of several more common models, and see how they perform. Of the metrics at the bottom of this section, the one with the best RSquared is the one we are likely to select.

```{r modelCompare, , message=FALSE, warning=FALSE}

set.seed(200)  # for reproducibility

train <- train %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%
  select(where(is.numeric))  # Ensure only numeric data is used

# Split the train data into new training and testing sets
set.seed(123)  # Ensure reproducibility
indexes <- createDataPartition(train$ph, p = 0.8, list = FALSE)
training_data <- train[indexes, ]
testing_data <- train[-indexes, ]

# Prepare predictor and response variables correctly
training_data$x <- data.frame(sapply(training_data[, -which(names(training_data) == "ph")], as.numeric))
training_data$y <- training_data$ph
testing_data$x <- data.frame(sapply(testing_data[, -which(names(testing_data) == "ph")], as.numeric))
testing_data$y <- testing_data$ph

# KNN
knnModel <- train(x = training_data$x, y = training_data$y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 5)
knnPred <- predict(knnModel, newdata = testing_data$x)
knnMetrics <- postResample(pred = knnPred, obs = testing_data$y)

# MARS
marsModel <- train(x = training_data$x, y = training_data$y,
                   method = "earth",
                   preProc = c("center", "scale"),
                   tuneLength = 5)
marsPred <- predict(marsModel, newdata = testing_data$x)
marsMetrics <- postResample(pred = marsPred, obs = testing_data$y)

# Neural Net
nnModel <- train(x = training_data$x, y = training_data$y,
                 method = "nnet",
                 preProcess = c("center", "scale"),
                 tuneLength = 5, 
                 trace = FALSE)
nnPred <- predict(nnModel, newdata = testing_data$x)
nnMetrics <- postResample(pred = nnPred, obs = testing_data$y)

# SVM
svmModel <- train(x = training_data$x, y = training_data$y,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneLength = 5)
svmPred <- predict(svmModel, newdata = testing_data$x)
svmMetrics <- postResample(pred = svmPred, obs = testing_data$y)

# Random Forest
## features added in for importance after first round assesment. 
trainControl <- trainControl(
  method = "cv", # <- this is how you get your model to cross validate!
  number = 3, # <- three fold validation
  verboseIter = FALSE,
  savePredictions = "final",
  returnResamp = "all"
)

tuneGrid <- expand.grid(
  mtry = c(2, floor(sqrt(ncol(training_data$x))))
)

rfModel <- train(
  x = training_data$x,
  y = training_data$y,
  method = "rf",
  trControl = trainControl,
  tuneGrid = tuneGrid
)
rfPred <- predict(rfModel, newdata = testing_data$x)
rfMetrics <- postResample(pred = rfPred, obs = testing_data$y)

# Linear Regression
lmModel <- train(x = training_data$x, y = training_data$y,
                 method = "lm",
                 preProcess = c("center", "scale"))
lmPred <- predict(lmModel, newdata = testing_data$x)
lmMetrics <- postResample(pred = lmPred, obs = testing_data$y)


train_imputed <- train_imputed %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%
  select(where(is.numeric))  # Ensure only numeric data is used

set.seed(123) 
trainIndex <- createDataPartition(train_imputed$ph, p = 0.75, list = FALSE)
train_imputed_train <- train_imputed[trainIndex, ]
train_imputed_test <- train_imputed[-trainIndex, ]

# Prepare predictor and response variables correctly
train_imputed_train$x <- data.frame(sapply(train_imputed_train[, -which(names(train_imputed_train) == "ph")], as.numeric))
train_imputed_train$y <- train_imputed_train$ph
train_imputed_test$x <- data.frame(sapply(train_imputed_test[, -which(names(train_imputed_test) == "ph")], as.numeric))
train_imputed_test$y <- train_imputed_test$ph

#Random Forest, slightly tuned 
trainControl2 <- trainControl(
  method = "cv", #<- use cross validation 
  number = 3,  # Reducing the number of cross-validation folds to 3 for faster execution
  verboseIter = FALSE  # Turn off iteration verbosity
)

# Create a tuning grid
tuneGrid2 <- expand.grid(
  mtry = c(2, floor(sqrt(ncol(training_data$x))))  # Suggested mtry values
)

# Train the model using the random forest method
rfModel_imputed <- train(
  x = train_imputed_train$x,
  y = train_imputed_train$y,
  method = "rf",
  trControl = trainControl2,
  tuneGrid = tuneGrid2
)
rfPred_imputed <- predict(rfModel_imputed, newdata = train_imputed_test$x)
rfMetrics_imputed  <- postResample(pred = rfPred_imputed, obs = train_imputed_test$y)



# Collecting all model performance metrics into a small df
modelPerformance <- data.frame(
  Model = c("KNN", "MARS", "Neural Network", "SVM", "Random Forest", "Linear Regression", "Bagimpute RF"),
  RMSE = c(knnMetrics[1], marsMetrics[1], nnMetrics[1], svmMetrics[1], rfMetrics[1], lmMetrics[1], rfMetrics_imputed[1]),
  Rsquared = c(knnMetrics[2], marsMetrics[2], nnMetrics[2], svmMetrics[2], rfMetrics[2], lmMetrics[2], rfMetrics_imputed[2]),
  MAE = c(knnMetrics[3], marsMetrics[3], nnMetrics[3], svmMetrics[3], rfMetrics[3], lmMetrics[3], rfMetrics_imputed[3] )
)

print(modelPerformance)
```

## Expanding Linear Regression
Though the model performance datframe suggests we use Random Forest, let's first double check linear regression because it is easy to understand. We'll exclude columns carbpressure, fillerlevel, ballinglvl, balling, and alchrel, all of which exhibit collinearity. As well and in the interest of brevity, we'll omit the background work where we discerned those columns should be removed. We'll do this just to see what another model thinks the important columns are. 

### LR Model Performance
We'll make adjustments, fit clean up the model, predict, and then see how far off the model was with predictions. As the results show, this cleaning up doesn't make much of a difference. 

```{r expandLinearRegression, message=FALSE, warning=FALSE}
# Filter the train data frame to exclude specified columns
train_filtered <- train[, !(colnames(train) %in% c("carbpressure", "fillerlevel", "ballinglvl", "balling", "alchrel"))]

# Split the data into new training and testing sets based on the filtered train data
set.seed(123)  # Ensure reproducibility
indexes <- createDataPartition(train_filtered$ph, p = 0.75, list = FALSE)
training_data_filtered <- train_filtered[indexes, ]
testing_data_filtered <- train_filtered[-indexes, ]

# Prepare predictor and response variables for the highly configured linear model
training_data_filtered$x <- data.frame(sapply(training_data_filtered[, -which(names(training_data_filtered) == "ph")], as.numeric))
training_data_filtered$y <- training_data_filtered$ph
testing_data_filtered$x <- data.frame(sapply(testing_data_filtered[, -which(names(testing_data_filtered) == "ph")], as.numeric))
testing_data_filtered$y <- testing_data_filtered$ph

# Fit the Linear Regression model on the filtered dataset
lmModelFiltered <- train(x = training_data_filtered$x, y = training_data_filtered$y,
                         method = "lm",
                         preProc = c("center", "scale"))

# Predict and calculate metrics
lmPredFiltered <- predict(lmModelFiltered, newdata = testing_data_filtered$x)
lmMetricsFiltered <- postResample(pred = lmPredFiltered, obs = testing_data_filtered$y)

# Add this LR metric
modelPerformance <- rbind(modelPerformance, data.frame(
  Model = "Linear Regression (Configured)",
  RMSE = lmMetricsFiltered[1],
  Rsquared = lmMetricsFiltered[2],
  MAE = lmMetricsFiltered[3]
))

print(modelPerformance)

```
### LR coefficients plot
Let's have a quick look at what LR thinks the more important variables are. 

```{r lrplot, message=FALSE, warning=FALSE}
coef_info <- coef(lmModelFiltered$finalModel)  # Extract model coefficients

coef_df <- data.frame( # Convert to df
  Variable = names(coef_info),
  Coefficient = coef_info
)

coef_df <- coef_df[-1, ] # Remove the intercept

library(ggplot2)
ggplot(coef_df, aes(x = reorder(Variable, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
  geom_col() +
  coord_flip() +
  labs(title = "Importance of Variables in LR Model",
       x = "Variables",
       y = "Coefficient Value") +
  theme_minimal()
```                  

## Expanding Random Forest
We know Random Forest is going to fit better, but before we predict, let's go through all the details and refine.

### Feature Importance
First, let's get a handle on which features seem the most important.

```{r expandRandomForest, message=FALSE, warning=FALSE}
importance_measures <- varImp(rfModel, scale = TRUE)
plot(importance_measures, main = "Feature Importance in Random Forest Model")
```
### Removing irrelevant variables
When we get rid of some of the extra and more irrelevant variables, Random Forest performs much better. Before we refit this model, we'll set an importance threshold get rid of any column that doesn't help stabilize the model. 

```{r rfRemoveColumms, message=FALSE, warning=FALSE, results='hide'}
# Function to clean and type
clean_data <- function(data, vars_to_remove) {
  data <- data[, !(names(data) %in% vars_to_remove)]
  data <- as.data.frame(data)
  data[] <- lapply(data, function(x) if(is.list(x)) unlist(x) else x)
  return(data)
}

# important features
importance_df <- as.data.frame(importance_measures$importance)
importance_df$Variable <- rownames(importance_df)
importance_df <- importance_df[order(importance_df$Overall), ]
# print(importance_df)

# cut off at less than importance_factor %
importance_factor <- 0.3
cutoff <- quantile(importance_df$Overall, importance_factor)
variables_to_remove <- importance_df$Variable[importance_df$Overall <= cutoff]
# print(variables_to_remove)

# Clean, and remove non-predictor variables such as 'x' and 'y'
training_data_updated <- clean_data(training_data, c(variables_to_remove, "x", "y", "ph"))
sapply(training_data_updated, class)

trainControl2 <- trainControl(
  method = "cv", # <- this is how you get your model to cross validate!
  number = 3, # <- three fold validation
  verboseIter = FALSE,
  savePredictions = "final",
  returnResamp = "all"
)

tuneGrid2 <- expand.grid(
  mtry = c(2, floor(sqrt(ncol(training_data$x))))
)

# Retrain the model with cleaned data
rfModel_updated <- train(
  x = training_data_updated,
  y = training_data$y,
  method = "rf",
  trControl = trainControl2,
  tuneGrid = tuneGrid2
)

# Prepare and clean prediction data
predictor_data_for_model <- clean_data(testing_data, c(variables_to_remove, "x", "y"))
sapply(predictor_data_for_model, class)  # Optional: Check classes of the prediction data columns

# Make predictions and evaluate the model
rfPred_updated <- predict(rfModel_updated, newdata = predictor_data_for_model)
rfMetrics_updated <- postResample(pred = rfPred_updated, obs = testing_data$y)

updated_model_performance <- data.frame(
  Model = paste0("Random Forest (w/importance_factor : ", importance_factor, ")"),
  RMSE = rfMetrics_updated[1],
  Rsquared = rfMetrics_updated[2],
  MAE = rfMetrics_updated[3]
)

modelPerformance <- rbind(modelPerformance, updated_model_performance)
modelPerformance
#Note that at this point, we now have two rf modles, rfModel and rfModelUpdated.
```

```{r}
print(variables_to_remove) #<- these were removed before we re-trained our model 
print(modelPerformance) #<- a big improvement
```

### Redsiduals Plot Review
The residuals appear randomly distributed around the zero line which is ideal. This suggests that the model does not suffer from non-constant variance. As well, there are no apparent patterns or trends in the residuals, which implies that the linearity between predictors and target is reasonably satisfied.  

```{r residualsPlot}
predictions <- predict(rfModel_updated, newdata = predictor_data_for_model)
residuals <- testing_data$y - predictions
residuals_df <- data.frame(Predicted = predictions, Residuals = residuals)
residuals_df <- na.omit(residuals_df)
ggplot(residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals")
```

### Imputation

Before we can predict, we need to do something with the missing fields from the predict_me data set. We'll use a simple linear regression model to find all those fields, go through the predictors, and fill those missing records out. Please note this is reusable code. 

```{r lrImputeMethod, message=FALSE, warning=FALSE}
impute_linear_regression <- function(data) {
    # For each column ... 
    for (col in names(data)) {
        # is numeric and any missing values
        if (is.numeric(data[[col]]) && any(is.na(data[[col]]))) {
            # Identify predictor columns, excluding the current column and any columns with missing values
            predictors <- setdiff(names(data), col)
            predictors <- predictors[!sapply(data[predictors], function(x) any(is.na(x)))]

            # Extract rows where the target column is not missing for training the model
            train_data <- data[!is.na(data[[col]]), predictors, drop = FALSE]
            train_target <- data[[col]][!is.na(data[[col]])]

            # if predictors and sufficient data, fit a linear model
            if (nrow(train_data) > 1 && length(predictors) > 0) {
                lm_model <- lm(train_target ~ ., data = train_data)

                # find indices of missing values in the target column
                na_indices <- which(is.na(data[[col]]))
                if (length(na_indices) > 0) {
                    # predict missing values
                    predictions <- predict(lm_model, newdata = data[na_indices, predictors, drop = FALSE])
                    # Replace missing values with the predictions
                    data[[col]][na_indices] <- predictions
                }
            } else {
                # Use median imputation as a fallback when not enough data is available for regression
                median_value <- median(train_target, na.rm = TRUE)
                data[[col]][is.na(data[[col]])] <- median_value
            }
        }
    }
    return(data)
}

```

## Prediction
Now that we have an imputation strategy, we can predict. Please note in this research we have to Random Forest models, rfModel, and rfModel_updated. The updated one is the one where we further refined by adjusting and removing columns to get a better fit. We'll be using that one. 

The output file will be saved to `with_predictions.csv`, and the prediction column is `predicted_ph.` 

```{r rePredict, message=FALSE, warning=FALSE, results='hide'}
all_vars_to_remove <- c("ph", variables_to_remove) 

predict_me_filled <- predict_me %>% 
  select(-all_of(all_vars_to_remove)) %>% 
  impute_linear_regression()

predicted_ph <- predict(rfModel_updated, newdata = predict_me_filled)
predict_me_filled$predicted_ph <- round(predicted_ph, 5)

predict_me_filled %>% ## one last cleanup. 
  mutate_if(is.numeric, round, digits = 5)
  
write.csv(predict_me_filled, "with_predictions.csv", row.names = FALSE)
```


